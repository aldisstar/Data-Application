# -*- coding: utf-8 -*-
"""Data Analysis - Module 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VYzeJH8mB9UO3iQ458Z2ZYXiGYisqGAy

Excercise 1.12.2 - Consider a binomial model where each of 31000 individuals has a probability of 0.00203 (or 2.03 per 1000) of dying due to breast cancer. Use the binomial pmf to calculate the probability that exactly 63 of the 31000 patients die of breast cancer.

(Enter a numerical answer accurate to at least 4 decimal places. You could compute the binomial coefficient in Python using the scipy.special.comb(n, k) command as part of the SciPy package. )
"""

from scipy.stats import binom
k = 63
n = 31000
p = 0.00203

ans = binom.pmf(k,n,p)
print(ans)

"""Excercise 2.7.2"""

import numpy as np
from scipy.stats import stats

X = np.array([ 0.9, -0.9, 4.3, 2.9, 1.2, 3.0 , 2.7, 0.6, 3.6, -0.5])
t_stats, p_value = stats.ttest_1samp(X, popmean=0)
p_value=p_value/2 # stats.ttest_1samp gives the ùëù value based on a two-sided test test so we halve it to obtain the ùëù value for a one-sided test.

print(p_value)

"""Lecture: Correlation and Least Squares Regression
Excercise 2.1
"""

import numpy as np
Xs = np.array([0.0339, 0.0423, 0.213, 0.257, 0.273, 0.273, 0.450, 0.503, 0.503, \
0.637, 0.805, 0.904, 0.904, 0.910, 0.910, 1.02, 1.11, 1.11, 1.41, \
1.72, 2.03, 2.02, 2.02, 2.02])

Ys = np.array([-19.3, 30.4, 38.7, 5.52, -33.1, -77.3, 398.0, 406.0, 436.0, 320.0, 373.0, \
93.9, 210.0, 423.0, 594.0, 829.0, 718.0, 561.0, 608.0, 1.04E3, 1.10E3, \
840.0, 801.0, 519.0])

mean_x = np.mean(Xs)
mean_y = np.mean(Ys)

st_x = np.std(Xs, ddof=1)
st_y = np.std(Ys, ddof=1)

COV = np.cov(Xs, Ys)[0, 1]

cof_ratio = np.corrcoef(Xs, Ys)[0, 1]
r = COV/(st_x*st_y)

print("mean_x, mean_y:")
print(mean_x,mean_y)
print("st_x,st_y:")
print(st_x,st_y)
print("COV:")
print(COV)
print("coef_ratio:")
print(cof_ratio), print(r)

X_cuad = Xs ** 2
cof_ratio_x_cuad = np.corrcoef(X_cuad, Ys)[0, 1]
print(cof_ratio_x_cuad)

# Calcular beta:
beta = r*st_y/st_x

# numerator = np.sum((Xs - mean_x) * (Ys - mean_y))
# denominator = np.sum((Xs - mean_x) ** 2)
# beta = numerator / denominator
print("beta:")
print(beta)

beta_0 = mean_y - beta * mean_x
print("beta0:")
print(beta_0)

"""5. Correcting simple nonlinear relationships"""

import numpy as np
import matplotlib.pyplot as plt

# Data
Xs = np.array([0.387, 0.723, 1.00, 1.52, 5.20, 9.54, 19.2, 30.1, 39.5])
Ys = np.array([0.241, 0.615, 1.00, 1.88, 11.9, 29.5, 84.0, 165.0, 248])

# Calculate the correlation coefficient (Pearson's r)
r = np.corrcoef(Xs, Ys)[0, 1]

# Create a scatter plot
plt.scatter(Xs, Ys)
plt.xlabel('Xs')
plt.ylabel('Ys')
plt.title('Scatter Plot of Xs vs. Ys')
plt.grid(True)

# Print the calculated correlation coefficient
print("Correlation coefficient (r):", r)

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import statsmodels.api as sm

Xs = np.array([ 0.387, 0.723, 1.00, 1.52, 5.20, 9.54, 19.2, 30.1, 39.5 ])

Ys = np.array([ 0.241, 0.615, 1.00, 1.88, 11.9, 29.5, 84.0, 165.0, 248 ])

plt.scatter(Xs, Ys, label="Data Points")

plt.xlabel("Xs")
plt.ylabel("Ys")
plt.legend("original")


correlation_coef = np.corrcoef(Xs, Ys)[0, 1]

rounded_correlation_coef = round(correlation_coef, 2)
print(f"Correlation Coef: {rounded_correlation_coef}")

plt.show()


# Perform linear regression
slope, intercept, r_value, p_value, std_err = stats.linregress(Xs, Ys)

# Calculate the predicted values using the linear regression equation
predicted_Ys = slope * Xs + intercept

# Calculate the residuals
residuals = Ys - predicted_Ys

# Create a scatter plot of the residuals
plt.scatter(Xs, residuals, label="Residuals")

# Add a horizontal line at y=0 for reference
plt.axhline(0, color='red', linestyle='--', linewidth=1, label="Residuals Mean")

# Add labels and a legend
plt.xlabel("Xs")
plt.ylabel("Residuals")
plt.legend("Residual vs X")

# Show the plot
plt.show()


# Create Q-Q plot for Xs
#plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sm.qqplot(Xs, line='s')
plt.title("Q-Q Plot for Xs")

# Create Q-Q plot for Ys
plt.subplot(1, 2, 2)
sm.qqplot(Ys, line='s')
plt.title("Q-Q Plot for Ys")

plt.tight_layout()
plt.show()

from scipy.stats import linregress

# Perform linear regression
slope, intercept, r_value, p_value, std_err = linregress(Xs, Ys)

# Calculate the predicted values and residuals
predicted_Y = intercept + slope * Xs
residuals = Ys - predicted_Y

# Create a scatter plot of residuals vs. X (or predicted values)
plt.scatter(Xs, residuals)
plt.xlabel('X')
plt.ylabel('Residuals')
plt.title('Residuals Plot')
plt.axhline(0, color='red', linestyle='--', linewidth=1)  # Add a horizontal line at y=0
plt.grid(True)

# Show the plot
plt.show()

# QQ plots

import statsmodels.api as sm
sm.qqplot(Xs, line='s')
plt.title("X distribution")
plt.show()

# Explore transformations

X_1 = np.log(Xs)
Y_1 = np.log(Ys)
# Perform linear regression
slope, intercept, r_value, p_value, std_err = linregress(X_1, Y_1)

# Calculate the predicted values and residuals
predicted_Y = intercept + slope * X_1
residuals = Y_1 - predicted_Y

# Create a scatter plot of residuals vs. X (or predicted values)
plt.scatter(Xs, residuals)
plt.xlabel('X')
plt.ylabel('Residuals')
plt.title('Residuals Plot')
plt.axhline(0, color='red', linestyle='--', linewidth=1)  # Add a horizontal line at y=0
plt.grid(True)

# Show the plot
plt.show()
print(slope, intercept)



"""5.4"""

import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy import stats

# Define the data
Xs = np.array([0.387, 0.723, 1.00, 1.52, 5.20, 9.54, 19.2, 30.1, 39.5])
Ys = np.array([0.241, 0.615, 1.00, 1.88, 11.9, 29.5, 84.0, 165.0, 248])

# Create a function to fit and plot the data with a specified transformation
def fit_and_plot(X, Y, transformation_name):
    # Apply the specified transformation to X and Y
    if transformation_name == "log":
        X_transformed = np.log(X)
        Y_transformed = np.log(Y)
    elif transformation_name == "sqrt":
        X_transformed = np.sqrt(X)
        Y_transformed = np.sqrt(Y)
    else:
        X_transformed = X
        Y_transformed = Y

    # Perform linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(X_transformed, Y_transformed)

    # Calculate the predicted values using the linear regression equation
    predicted_Ys = slope * X_transformed + intercept

    # Calculate the residuals
    residuals = Y_transformed - predicted_Ys

    # Create a Q-Q plot for the residuals
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sm.qqplot(residuals, line='s')
    plt.title(f"Q-Q Plot for Residuals ({transformation_name} transformed)")

    # Create a scatter plot of the transformed data and the regression line
    plt.subplot(1, 2, 2)
    plt.scatter(X_transformed, Y_transformed, label="Transformed Data")
    plt.plot(X_transformed, predicted_Ys, color='red', label="Regression Line")
    plt.xlabel(f"{transformation_name.capitalize()} Transformed X")
    plt.ylabel(f"{transformation_name.capitalize()} Transformed Y")
    plt.title(f"Linear Regression ({transformation_name} transformed)")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Try different transformations and compare the results
transformations_to_try = ["none", "log", "sqrt"]

for transformation in transformations_to_try:
    fit_and_plot(Xs, Ys, transformation)

"""5.5"""

# import numpy as np
# from scipy import stats

# # Define the data
# Xs = np.array([0.387, 0.723, 1.00, 1.52, 5.20, 9.54, 19.2, 30.1, 39.5])
# Ys = np.array([0.241, 0.615, 1.00, 1.88, 11.9, 29.5, 84.0, 165.0, 248])

# # Apply the transformation to Ys
# Y_transformed = np.log(Ys)

# # Perform linear regression between log(Y) and X
# slope, intercept, r_value, p_value, std_err = stats.linregress(Xs, Y_transformed)

# # The slope "K" is the coefficient obtained from the linear regression
# K = slope

# print(f"The value of K is approximately: {K:.2f}")

"""6 - Multiple linear model

LogPlanetMass is the logarithm of the observed exoplanet's mass in units of Jupiter's mass. A LogPlanetMass of zero is an exoplanet with the same mass as Jupiter. Jupiter is used as a convenient comparison, as large gas giants are the most easily detected, and thus most commonly observed, kind of exoplanet.

LogPlanetRadius is the logarithm of the observed exoplanet's radius in units of Jupiter's radius, for much the same reason.

LogPlanetOrbit is the logarithm of the observed planet's semi-major axis of orbit, in units of AU.

StarMetallicity is the relative amount of metals observed in the parent star. It is equal to the logarithm of the ratio of the observed abundance of metal to the observed abundance of metal in the Sun. The Sun is a quite average star, so it serves as a good reference point. The most common metal to measure is Iron, but astronomers define any element that isn't Hydrogen or Helium as a metal.

LogStarMass is the logarithm of the parent star's mass in units of the Sun's mass.

LogStarAge is the logarithm of the parent star's age in giga-years.

Let ùë¶ be the vector of LogPlanetMass. Then place the remaining variables so that they form columns of ùëã. You should also insert an additional column of ones to allow for an intercept, thus you will have six ùõΩ parameters.

The lay-out of ùëã should be, left-to-right: intercept, LogPlanetRadius, LogPlanetOrbit, StarMetallicity, LogStarMass, LogStarAge.

Find the estimate of ùú∑ using multi linear least squares regression. What is ùú∑ÃÇ  to three significant figures in each element?
"""

import numpy as np
LogPlanetMass = np.array([-0.31471074,  1.01160091,  0.58778666,  0.46373402, -0.01005034,
         0.66577598, -1.30933332, -0.37106368, -0.40047757, -0.27443685,
         1.30833282, -0.46840491, -1.91054301,  0.16551444,  0.78845736,
        -2.43041846,  0.21511138,  2.29253476, -2.05330607, -0.43078292,
        -4.98204784, -0.48776035, -1.69298258, -0.08664781, -2.28278247,
         3.30431931, -3.27016912,  1.14644962, -3.10109279, -0.61248928])

LogPlanetRadius = np.array([ 0.32497786,  0.34712953,  0.14842001,  0.45742485,  0.1889661 ,
         0.06952606,  0.07696104,  0.3220835 ,  0.42918163, -0.05762911,
         0.40546511,  0.19227189, -0.16251893,  0.45107562,  0.3825376 ,
        -0.82098055,  0.10436002,  0.0295588 , -1.17921515,  0.55961579,
        -2.49253568,  0.11243543, -0.72037861,  0.36464311, -0.46203546,
         0.13976194, -2.70306266,  0.12221763, -2.41374014,  0.35627486])

LogPlanetOrbit = np.array([-2.63108916, -3.89026151, -3.13752628, -2.99633245, -3.12356565,
        -2.33924908, -2.8507665 , -3.04765735, -2.84043939, -3.19004544,
        -3.14655516, -3.13729584, -3.09887303, -3.09004295, -3.16296819,
        -2.3227878 , -3.77661837, -2.52572864, -4.13641734, -3.05018846,
        -2.40141145, -3.14795149, -0.40361682, -3.2148838 , -2.74575207,
        -3.70014265, -1.98923527, -3.35440922, -1.96897409, -2.99773428])

StarMetallicity = np.array([ 0.11 , -0.002, -0.4  ,  0.01 ,  0.15 ,  0.22 , -0.01 ,  0.02 ,
        -0.06 , -0.127,  0.   ,  0.12 ,  0.27 ,  0.09 , -0.077,  0.3  ,
         0.14 , -0.07 ,  0.19 , -0.02 ,  0.12 ,  0.251,  0.07 ,  0.16 ,
         0.19 ,  0.052, -0.32 ,  0.258,  0.02 , -0.17 ])

LogStarMass = np.array([ 0.27002714,  0.19144646, -0.16369609,  0.44468582,  0.19227189,
         0.01291623,  0.0861777 ,  0.1380213 ,  0.49469624, -0.43850496,
         0.54232429,  0.02469261,  0.07325046,  0.42133846,  0.2592826 ,
        -0.09431068, -0.24846136, -0.12783337, -0.07364654,  0.26159474,
         0.07603469, -0.07796154,  0.09440068,  0.07510747,  0.17395331,
         0.28893129, -0.21940057,  0.02566775, -0.09211529,  0.16551444])

LogStarAge = np.array([ 1.58103844,  1.06471074,  2.39789527,  0.72754861,  0.55675456,
         1.91692261,  1.64865863,  1.38629436,  0.77472717,  1.36097655,
         0.        ,  1.80828877,  1.7837273 ,  0.64185389,  0.69813472,
         2.39789527, -0.35667494,  1.79175947,  1.90210753,  1.39624469,
         1.84054963,  2.19722458,  1.89761986,  1.84054963,  0.74193734,
         0.55961579,  1.79175947,  0.91629073,  2.17475172,  1.36097655])

N = 30

intercept_column = np.ones_like(LogPlanetMass)

# Create the design matrix X by stacking columns horizontally
X = np.column_stack((intercept_column, LogPlanetRadius, LogPlanetOrbit, StarMetallicity, LogStarMass, LogStarAge))
# print(X)

y = LogPlanetMass

# Calculate Œ≤ÃÇ using multi-linear least squares regression
beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]

print(beta_hat)

# otra forma:
import numpy.linalg
# The beta estimator using the matrix inversion formula
betaVec = numpy.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

"""	ùëáùëó = ùõΩÃÇ ùëó‚àí0ùúéŒ£ùëó(ùëÅ‚àíùëù)ùúéÃÇ 2ùúé2ùëÅ‚àíùëù‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚àö
 	 	= ùõΩÃÇùëó / (ùúéÃÇ Œ£ùëó)
    We denote the ùëó-th diagonal element of (ùëã‚ä∫ùëã)‚àí1 by Œ£2ùëó.
"""

residuals = y - np.dot(X, beta_hat)
p = 5

# Calculate the sum of squared residuals
residuals = y - np.dot(X, beta_hat)
ssr = np.sum(residuals**2)

# Calculate the estimated variance of the residuals
sigma_hat_squared = ssr / (N - p)

print("Estimated variance of residuals (sigma^2):", sigma_hat_squared)

from scipy.stats import t

# Number of coefficients (including the intercept)
num_coefficients = len(beta_hat)

# Initialize lists to store t-statistics and p-values
t_statistics = []
p_values = []

# Loop through all coefficients
for j in range(num_coefficients):
    # Extract the jth coefficient from the estimated coefficients vector
    beta_j = beta_hat[j]

    # Extract the jth column of the design matrix
    X_j = X[:, j]

    # Calculate the standard error of the jth coefficient
    se_beta_j = np.sqrt(sigma_hat_squared * np.linalg.inv(np.dot(X.T, X))[j, j])

    # Calculate the t-statistic
    t_statistic = beta_j / (se_beta_j / np.sqrt(N - X.shape[1]))

    # Calculate the degrees of freedom (N - p, where p is the number of predictors)
    df = N - X.shape[1]

    # Calculate the two-tailed p-value
    p_value = 2 * (1 - t.cdf(np.abs(t_statistic), df))

    # Append the results to the lists
    t_statistics.append(t_statistic)
    p_values.append(p_value)

# Print the t-statistics and p-values for all coefficients
for j, (t_stat, p_val) in enumerate(zip(t_statistics, p_values)):
    print(f"Coefficient {j}:")
    print(f"t-statistic (T_{j}): {t_stat}")
    print(f"p-value: {p_val}")

"""5.3"""

import sympy as sp

# Define the variables
x, y, a, b = sp.symbols('x y a b')

# Define the loss function
f = (a**2 * x - y)**2 + (b + x + y)**2

# Calculate the partial derivatives with respect to x and y
df_dx = sp.diff(f, x)
df_dy = sp.diff(f, y)

# Set the partial derivatives equal to zero
eq1 = sp.Eq(df_dx, 0)
eq2 = sp.Eq(df_dy, 0)

# Solve the system of equations for x and y
critical_points = sp.solve((eq1, eq2), (x, y))

# Print the critical points in terms of a and b
print("Critical Points in terms of a and b:")

print(critical_points)

"""### Homework 1.2

The file gamma-ray.csv contains a small quantity of data collected from the Compton Gamma Ray Observatory, a satellite launched by NASA in 1991 (http://cossc.gsfc.nasa.gov/). For each of 100 sequential time intervals of variable lengths (given in seconds), the number of gamma rays originating in a particular area of the sky was recorded. You would like to check the assumption that the emission rate is constant.
"""

import pandas as pd

file_name = 'gamma-ray.csv'

df = pd.read_csv(file_name)

df

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(df['seconds'], df['count'], c='blue', marker='o', label='Data Points')
plt.title('Scatter Plot of Data')
plt.xlabel('Seconds')
plt.ylabel('Count')
plt.legend()
plt.grid(True)
plt.show()

df['rate'] = df['count'] / df['seconds']
df

# Calculate MLEs for each interval
df['MLE_lambda'] = df['count'] / df['seconds']

# Display the results
print(df)

# Calculate the average of MLE_lambda
average_mle_lambda = df['count'].sum()/df['seconds'].sum()

# Print the average
print(f'Average MLE_lambda: {average_mle_lambda}')

import numpy as np
import pandas as pd
import scipy.stats
from scipy.stats import chi2
from scipy.stats import poisson


# Calculate the null likelihood
null_lambda = df['count'].sum() / df['seconds'].sum()
param = null_lambda * df['seconds']
null_likelihoods = []
for i in range(100):
  count = df.at[i, 'count']
  param_i = param[i]
  null_likelihood_i = poisson.pmf(count, param_i)
  null_likelihoods.append(null_likelihood_i)

# Calculate the likelihood under the alternate hypothesis
alternate_likelihoods = []

for i in range(100):
    lambda_i = df.at[i, 'rate']
    param = lambda_i * df.at[i, 'seconds']
    count = df.at[i, 'count']
    likelihood_i = poisson.pmf(count, param)
    alternate_likelihoods.append(likelihood_i)

# LRT
test_statistic = -2 * np.log(max(np.array(null_likelihoods)) / max(np.array(alternate_likelihoods)))

# Calculate the p-value
degrees_of_freedom = 99
p_value = 1 - chi2.cdf(test_statistic, df=degrees_of_freedom)

print(null_likelihoods)
print(alternate_likelihoods)
print(f"Test Statistic: {test_statistic}")
print(f"P-value: {p_value}")

import numpy as np
import scipy.stats as stats

# Extract the data from the DataFrame column
data = df['rate'].values

# Define the null hypothesis (common lambda for all values)
lambda_null = np.mean(data)

# Define the likelihood function under the null hypothesis
def likelihood_null(lambda_value):
    return np.prod(stats.poisson.pmf(data, lambda_value))

# Define the alternative hypothesis (individual lambdas)
# You can define different lambdas for your specific alternative hypothesis

# Calculate the likelihood under the alternative hypothesis
# Here, we assume lambda1 != lambda2 != ... != lambda99
def likelihood_alternative(lambda_values):
    return np.prod([stats.poisson.pmf(x, lambda_values[i]) for i, x in enumerate(data)])

# Calculate the likelihood ratios
LR = likelihood_alternative([lambda_null] * len(data)) / likelihood_null(lambda_null)

# Calculate the test statistic (-2 * log likelihood ratio)
LRT_statistic = -2 * np.log(LR)

# Calculate the degrees of freedom
degrees_of_freedom = 99

# Calculate the p-value using the chi-square distribution
p_value = 1 - stats.chi2.cdf(LRT_statistic, df=degrees_of_freedom)

print("Likelihood Ratio Test Statistic:", LRT_statistic)
print("p-value:", p_value)

import numpy as np
import scipy.stats as stats

# Extract the data from the DataFrame column
data = df['rate'].values

# Check for and remove any NaN or infinite values in the data
data = data[~np.isnan(data) & np.isfinite(data)]

# Check if there is any data left after removing invalid values
if len(data) == 0:
    print("No valid data points remaining.")
else:
    # Define the null hypothesis (common lambda for all values)
    lambda_null = np.mean(data)

    # Define the likelihood function under the null hypothesis
    def likelihood_null(lambda_value):
        return np.prod(stats.poisson.pmf(data, lambda_value))

    # Define the alternative hypothesis (individual lambdas)
    # You can define different lambdas for your specific alternative hypothesis

    # Calculate the likelihood under the alternative hypothesis
    # Here, we assume lambda1 != lambda2 != ... != lambda99
    def likelihood_alternative(lambda_values):
        return np.prod([stats.poisson.pmf(x, lambda_values[i]) for i, x in enumerate(data)])

    # Calculate the likelihood ratios
    LR = likelihood_alternative([lambda_null] * len(data)) / likelihood_null(lambda_null)

    # Calculate the test statistic (-2 * log likelihood ratio)
    LRT_statistic = -2 * np.log(LR)

    # Calculate the degrees of freedom
    degrees_of_freedom = len(data) - 1

    # Calculate the p-value using the chi-square distribution
    p_value = 1 - stats.chi2.cdf(LRT_statistic, df=degrees_of_freedom)

    print("Likelihood Ratio Test Statistic:", LRT_statistic)
    print("p-value:", p_value)

# import numpy as np
# from scipy.stats import chi2

# # Likelihoods under the null and alternative models (replace with your actual likelihoods)
# likelihood_null = ...
# likelihood_alternative = ...

# # Calculate the likelihood ratio
# LR = likelihood_alternative / likelihood_null

# # Calculate the LRT statistic
# LRT = -2 * np.log(LR)

# # Degrees of freedom (often equal to the difference in the number of parameters)
# degrees_of_freedom = 99

# # Calculate the p-value using the chi-square distribution
# p_value = 1 - chi2.cdf(LRT, df=degrees_of_freedom)

# print("Likelihood Ratio Statistic:", LRT)
# print("p-value:", p_value)

"""### Problem 1.4"""

# import pandas as pd
# raw_url = 'https://github.com/vschiavo/Data_Analysis/blob/main/golub.csv'


# Read the CSV file into a DataFrame
# data = pd.read_csv(raw_url)

file_name_1 = 'golub.csv'

data = pd.read_csv(file_name_1)

data

file_name_2 = 'golub_cl.csv'

data_cl = pd.read_csv(file_name_2)

data_cl
# 1 a 27 corresponden a pacientes con ALL y 28 a 38 pacientes con AML

ALL_data = data.iloc[:, :28]
ALL_data = ALL_data.drop(ALL_data.columns[0], axis=1)

# Select the last 11 columns
AML_data = data.iloc[:, -11:]

ALL_data

import numpy as np
import scipy.stats as stats

# Initialize an empty array to store p-values for each gene
p_values = []

# Define your significance level (alpha)
alpha = 0.05

# Loop through each row (gene) and calculate the Welch's t-test and p-value
for gene_index in range(len(ALL_data.index)):
    t_statistic, p_value = stats.ttest_ind(ALL_data.iloc[gene_index], AML_data.iloc[gene_index], equal_var=False)
    p_values.append(p_value)

# Convert the p-values list to a numpy array for further analysis
p_values_array = np.array(p_values)

# Count the number of significant genes (p-values below alpha)
num_significant_genes = np.sum(p_values_array < alpha)

print("Number of significantly associated genes:", num_significant_genes)

# Holm-Bonferroni Correction:

import statsmodels.stats.multitest as multi

p_values = []

# Loop through each row (gene) and calculate the Welch's t-test and p-value
for gene_index in range(len(ALL_data.index)):
    t_statistic, p_value = stats.ttest_ind(ALL_data.iloc[gene_index], AML_data.iloc[gene_index], equal_var=False)
    p_values.append(p_value)

# Convert the p-values list to a numpy array for further analysis
p_values_array = np.array(p_values)

# Apply Holm-Bonferroni correction
significant_genes_holm = multi.multipletests(p_values_array, method='holm')[0]

# Count the number of significant genes after Holm-Bonferroni correction
num_significant_genes_holm = np.sum(significant_genes_holm)

print("Number of significantly associated genes (Holm-Bonferroni correction):", num_significant_genes_holm)

# Benjamini-Hochberg Correction:

p_values = []

# Loop through each row (gene) and calculate the Welch's t-test and p-value
for gene_index in range(len(ALL_data.index)):
    t_statistic, p_value = stats.ttest_ind(ALL_data.iloc[gene_index], AML_data.iloc[gene_index], equal_var=False)
    p_values.append(p_value)

# Convert the p-values list to a numpy array for further analysis
p_values_array = np.array(p_values)

# Apply Benjamini-Hochberg correction
significant_genes_bh = multi.multipletests(p_values_array, method='fdr_bh')[0]

# Count the number of significant genes after Benjamini-Hochberg correction
num_significant_genes_bh = np.sum(significant_genes_bh)

print("Number of significantly associated genes (Benjamini-Hochberg correction):", num_significant_genes_bh)

"""### Problem 1.6


"""

data_x = pd.read_csv("syn_X.csv", header = None)
data_y = pd.read_csv("syn_y.csv", header = None)

data_x

data_y

import numpy as np

# Add a column of ones to data_x for the intercept term
ones_column = np.ones((data_x.shape[0], 1))
X = np.hstack((ones_column, data_x))

# Compute the OLS estimator
X_transpose = np.transpose(X)
XtX = np.dot(X_transpose, X)
Xty = np.dot(X_transpose, data_y)

# XtX * beta_hat = Xty
beta_hat = np.linalg.solve(XtX, Xty)

# The beta_hat variable now contains the estimated coefficients
print(beta_hat)
# print(X)

X = np.array(X)
y = np.array(data_y)

print(X.shape)
print(y.shape)
# beta, num_iterations = gradient_descent(X, y, step_size=0.01, precision=1e-6)

import numpy as np

def gradient_descent(X, y, step_size, precision):
    num_samples, num_features = X.shape
    beta = np.zeros((num_features, 1))  # Initial guess for beta
    prev_loss = float('inf')

    while True:
        # Calculate the predicted y values
        y_pred = np.dot(X, beta)

        # Calculate the gradient of the loss function
        gradient = -2 * np.dot(X.T, (y - y_pred))

        # Update beta
        beta -= step_size * gradient

        # Calculate the loss
        loss = np.mean((y - y_pred) ** 2)

        # Check for convergence
        if abs(prev_loss - loss) < precision:
            break

        prev_loss = loss

    return beta

# Load your data
data_x = pd.read_csv("syn_X.csv", header=None)
data_y = pd.read_csv("syn_y.csv", header=None)

# Add a column of ones to data_x for the intercept term
ones_column = np.ones((data_x.shape[0], 1))
X = np.hstack((ones_column, data_x))

# Convert data_y to a NumPy array
y = np.array(data_y)

# Test different step sizes
step_sizes = [0.001, 0.01, 0.1, 0.5, 1.0]

for step_size in step_sizes:
    beta_hat = gradient_descent(X, y, step_size, precision=1e-6)
    print(f"Step Size: {step_size}, Beta: {beta_hat.flatten()}")

# Choose the optimal step size based on the fewest iterations before convergence and desired precision.

import numpy as np

def gradient_descent(X, y, step_size, precision):
    """
    Perform gradient descent to minimize the least squares cost function.
    Parameters:
    - X: Design matrix (numpy array).
    - y: Target vector (numpy array).
    - step_size: Learning rate (float).
    - precision: Threshold for convergence (float).
    Returns:
    - beta: Estimated coefficients (numpy array).
    - num_iterations: Number of iterations until convergence (int).
    """
    num_samples, num_features = X.shape
    beta = np.zeros((num_features, 1))  # Initialize coefficients to zeros
    previous_cost = float('inf')
    num_iterations = 0

    while True:
        # Calculate the predicted values
        y_pred = np.dot(X, beta)

        # Calculate the gradient of the cost function
        # gradient = (-2 / num_samples) * np.dot(X.T, (y - y_pred))
        gradient = -2 * np.dot(X.T, (y - y_pred))

        # Update coefficients using gradient descent
        beta -= step_size * gradient

        # Calculate the new cost
        cost = np.sum((y - y_pred) ** 2)

        # Check for convergence
        if abs(previous_cost - cost) < precision:
            break

        previous_cost = cost
        num_iterations += 1

    return beta, num_iterations

# Example usage:
# beta, num_iterations = gradient_descent(X, y, step_size=0.01, precision=1e-6)

step_table = [0.0042, 0.0043, 0.0044, 0.00442, 0.00445, 0.0045, 0.0046]

for step_size in step_table:
  beta, num_iterations = gradient_descent(X, y, step_size, precision=10**(-6))
  print(f"step_size: {step_size}, beta = {beta}, num_iter: {num_iterations}")

import zipfile
import numpy as np

# returns a 3-tuple of (list of city names, list of variable names, numpy record array with each variable as a field)
def read_mortality_csv(zip_file):
  import io
  import csv
  fields, cities, values = None, [], []
  with io.TextIOWrapper(zip_file.open('data_and_materials/mortality.csv')) as wrap:
    csv_reader = csv.reader(wrap, delimiter=',', quotechar='"')
    fields = next(csv_reader)[1:]
    for row in csv_reader:
      cities.append(row[0])
      values.append(tuple(map(float, row[1:])))
  dtype = np.dtype([(name, float) for name in fields])
  return cities, fields, np.array(values, dtype=dtype).view(np.recarray)

with zipfile.ZipFile("statsreview_release1.zip") as zip_file:
  m_cities, m_fields, m_values = read_mortality_csv(zip_file)

type(m_values)

print(m_fields)

m_values[0]

import pandas as pd
import numpy as np

# Sample data as a NumPy recarray
data = np.array(m_values, dtype=[('col1', float), ('col2', float), ('col3', float), ('col4', float), ('col5', float), ('col6', float), ('col7', float), ('col8', float), ('col9', float), ('col10', float), ('col11', float), ('col12', float), ('col13', float), ('col14', float), ('col15', float)])

# Convert the recarray to a Pandas DataFrame
df = pd.DataFrame(data)

df.columns = ['Mortality', 'JanTemp', 'JulyTemp', 'RelHum', 'Rain', 'Educ', 'Dens', 'NonWhite', 'WhiteCollar', 'Pop', 'House', 'Income', 'HC', 'NOx', 'SO2']

# Display the resulting DataFrame
print(df)

import matplotlib.pyplot as plt

# Iterate through each column and create scatter plots
for column in df.columns[1:]:  # Skip 'Mortality' column
    plt.figure(figsize=(8, 6))  # Set the figure size
    plt.scatter(df[column], df['Mortality'], alpha=0.5)
    plt.title(f'Scatter Plot of {column} vs Mortality')
    plt.xlabel(column)
    plt.ylabel('Mortality')
    plt.grid(True)
    plt.show()

y = df['Mortality']
X = df.drop('Mortality', axis=1)

X = np.array(X)
y = np.array(y)

# # Standardize the features in X, y
X_s = (X - X.mean(axis=0)) / X.std(axis=0)
y_s = (y - y.mean()) / y.std()

# Add bias term to X_s
X_s = np.c_[np.ones(X_s.shape[0]), X_s]

beta = np.linalg.inv(X_s.T.dot(X_s)).dot(X_s.T).dot(y_s)

# Rounding the values to 2 decimal points
rounded_beta = np.round(beta, 2)
joined_string = ','.join(map(str, rounded_beta))
print(joined_string)

import numpy as np
import pandas as pd

X = np.array(X_1)
y = np.array(y)

# Standardize the features in X,y
X_s = (X - X.mean()) / X.std()
y_mean = y.mean()
y_std = y.std()
y_s = (y - y_mean) / y_std

# Define your gradient descent function
def gradient_descent(X, y, step_size, precision):
    num_samples, num_features = X.shape
    beta = np.zeros(num_features)  # Initialize coefficients to zeros as a row vector
    previous_cost = float('inf')
    num_iterations = 0

    while True:
        y_pred = np.dot(X, beta)
        gradient = -2 * np.dot(X.T, (y - y_pred))
        beta -= step_size * gradient

        cost = np.sum((y - y_pred) ** 2)

        if abs(previous_cost - cost) < precision:
            break

        previous_cost = cost
        num_iterations += 1

    return beta, num_iterations

step_table = [0.00055]

for step_size in step_table:
    beta, num_iterations = gradient_descent(X_s, y_s, step_size, precision=10**(-6))
    print(f"step_size: {step_size}, beta = {beta}, num_iter: {num_iterations}")